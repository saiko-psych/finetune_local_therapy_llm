# Finetuning eines Large Language Models fÃ¼r therapeutische Anwendungen

## ğŸ“‹ ProjektÃ¼bersicht

Dieses Projekt untersucht die Wirksamkeit des Finetunings von Large Language Models (LLMs) fÃ¼r therapeutische GesprÃ¤che durch den Vergleich eines Original Gemma-3-4B Modells mit einer selbst trainierten Version auf realen Mental Health Counseling Daten.

## ğŸ¯ Forschungsziel

**Hauptfragestellung:** Kann durch Finetuning die QualitÃ¤t therapeutischer Antworten eines LLMs signifikant verbessert werden - und das mit einem Modell, das datenschutzkonform lokal auf Consumer-Hardware lÃ¤uft?

**Motivation fÃ¼r lokale Modelle:**
- **Datenschutz:** Sensible therapeutische GesprÃ¤che bleiben vollstÃ¤ndig lokal
- **VerfÃ¼gbarkeit:** Keine AbhÃ¤ngigkeit von Cloud-APIs oder Internetverbindung
- **Kosten:** Keine laufenden API-Kosten nach einmaligem Setup
- **Kontrolle:** VollstÃ¤ndige Kontrolle Ã¼ber Modellverhalten und Updates
- **ZugÃ¤nglichkeit:** Demokratisierung von KI-Tools fÃ¼r therapeutische Anwendungen

**Hypothese:** Ein auf therapeutischen GesprÃ¤chsdaten finegetuntes 4B-Parameter Modell kann lokal auf Standard-Hardware (8-16GB GPU) betrieben werden und dabei hÃ¶here semantische Ã„hnlichkeit zu echten Therapeutenantworten zeigen als das ursprÃ¼ngliche Basismodell.

## ğŸ”¬ Methodologie

### 1. Datengrundlage

**PrimÃ¤rdataset (Finetuning):**
- `entfane/psychotherapy` - Strukturierte therapeutische GesprÃ¤che mit Metadaten
- EnthÃ¤lt Informationen zu Krankheitsbild und Therapiestadium
- Aufbereitung: Extraktion von Kontext-Response-Paaren mit Metainformationen

**Evaluationsdataset:**
- `Amod/mental_health_counseling_conversations` - 3.512 echte Therapeut-Patient GesprÃ¤che
- Zwei Testbedingungen:
  - **Gefiltert:** 50-1000 Zeichen (Context), 50-2000 Zeichen (Response)
  - **Ungefiltert:** ZufÃ¤llige Auswahl aus allen Daten
- Sample-GrÃ¶ÃŸe: 200 pro Bedingung = 400 total

### 2. Technische Implementierung

#### Modell-Setup: Lokale Deployment-Strategie
```python
# Basismodell: Google Gemma-3-4B-IT
# Bewusste Wahl eines kleineren Modells fÃ¼r lokale Nutzung
model_id = "google/gemma-3-4b-it"

# 4-Bit Quantisierung fÃ¼r Consumer-Hardware KompatibilitÃ¤t
BitsAndBytesConfig:
- 4-bit Quantisierung (NF4)     # Reduziert VRAM von ~16GB auf ~4GB
- Float16 Compute Dtype         # Weitere Memory-Optimierung
- CPU Offloading aktiviert      # Nutzt System-RAM als Backup
- Double Quantisation           # ZusÃ¤tzliche Kompression
```

**Warum Gemma-3-4B statt grÃ¶ÃŸerer Modelle?**
- **Hardware-Anforderungen:** LÃ¤uft auf 8GB GPU (RTX 3070/4060 Ti)
- **Inference-Geschwindigkeit:** ~50-100 Tokens/Sekunde auf Consumer-Hardware
- **Speicherbedarf:** ~4GB VRAM quantisiert vs. ~60GB+ fÃ¼r 70B Modelle
- **Energieeffizienz:** Lokal betreibbar ohne Serverfarm

#### LoRA Konfiguration
```python
LoraConfig:
- r=16 (Rank)
- lora_alpha=32
- target_modules=["q_proj", "v_proj"]
- lora_dropout=0.05
- task_type="CAUSAL_LM"
```

#### Training-Parameter
```python
TrainingArguments:
- Batch Size: 1 (mit Gradient Accumulation)
- Learning Rate: 1e-4
- Epochs: 10
- Mixed Precision: FP16
- LR Scheduler: Linear mit Warmup
- Early Stopping verfÃ¼gbar
```

### 3. Datenaufbereitung

#### Kontext-Extraktion mit Metadaten
Die Trainingsdaten werden strukturiert aufbereitet:
- **Metainformationen:** `[Illness: {illness}] [Stage: {stage}]`
- **Kontextaufbau:** Akkumulation der GesprÃ¤chshistorie
- **Pairs-Extraktion:** Kontext â†’ Therapeut-Response Paare

#### Tokenisierung
- Max Length: 1024 Token
- Padding: "max_length"
- Truncation: True
- Labels = Input IDs (Causal Language Modeling)

### 4. Experimentelles Design

**Evaluationsprozess:**
1. **Input:** Patient Context aus Evaluationsdataset
2. **Generation:** 
   - Original Model â†’ Response A
   - Finetuned Model â†’ Response B
3. **Ground Truth:** Echte Therapeut-Antwort
4. **Vergleich:** Cosine Similarity (TF-IDF basiert)

**Metriken:**
- Original vs Ground Truth
- Finetuned vs Ground Truth  
- Original vs Finetuned

### 5. Robustheit & Reproduzierbarkeit

**Fehlerbehandlung:**
- Checkpoint System (alle 10 Samples)
- 3 Retry-Versuche bei API-Fehlern
- Automatische Crash Recovery
- VollstÃ¤ndiges Logging

**DatenintegritÃ¤t:**
- Deterministische Seeds (42)
- Versionskontrolle aller Parameter
- Reproduzierbare Train/Test Splits

## ğŸ“Š Evaluationsstrategie

### Quantitative Metriken

**Cosine Similarity Analysis:**
- **Basis:** TF-IDF Vektorisierung
- **Vergleiche:** 3 pro Sample (1200 total)
- **Aggregation:** Mittelwerte, Standardabweichungen

**Statistische Tests:**
- Verbesserungsraten (Anteil Samples mit Similarity-Gain > 0)
- EffektgrÃ¶ÃŸen zwischen gefilterten/ungefilterten Bedingungen
- Signifikanztests fÃ¼r Modellunterschiede

### Visualisierung
- Verteilungsplots der Similarity-Scores
- Vergleichsgrafiken Original vs Finetuned
- Scatter-Plots fÃ¼r Korrelationsanalyse
- Box-Plots fÃ¼r VariabilitÃ¤tsvergleiche

## ğŸ› ï¸ Technische Anforderungen - Consumer-Hardware Ready

### Minimum Hardware (Budget-Setup)
- **GPU:** RTX 3060 12GB / RTX 4060 Ti 16GB / RX 6700 XT 12GB
- **RAM:** 16GB System-RAM (8GB fÃ¼r Modell-Offloading)
- **Storage:** 15GB freier Speicher (Modell + Checkpoints)
- **GeschÃ¤tzte Kosten:** ~400-600â‚¬ gebrauchte GPU

### Empfohlene Hardware (Komfort-Setup)  
- **GPU:** RTX 3070/4070 (8GB+) oder RTX 3080/4080 (10GB+)
- **RAM:** 32GB System-RAM (fÃ¼r grÃ¶ÃŸere Batch-Sizes)
- **Storage:** SSD mit 50GB+ freiem Speicher
- **GeschÃ¤tzte Kosten:** ~600-1000â‚¬ gebrauchte GPU

### Performance-Vergleich lokaler Betrieb
```
Inference-Geschwindigkeit (Consumer-Hardware):
â”œâ”€â”€ RTX 3060 12GB:  ~30-50 Tokens/Sekunde
â”œâ”€â”€ RTX 3070 8GB:   ~40-70 Tokens/Sekunde  
â”œâ”€â”€ RTX 4070 12GB:  ~60-100 Tokens/Sekunde
â””â”€â”€ RTX 4080 16GB:  ~80-120 Tokens/Sekunde

Vergleich Cloud-APIs:
â”œâ”€â”€ GPT-4: ~20-40 Tokens/Sekunde (+ Latenz + Kosten)
â”œâ”€â”€ Claude: ~15-30 Tokens/Sekunde (+ Latenz + Kosten)
â””â”€â”€ Lokales 4B: ~50-100 Tokens/Sekunde (keine Latenz, keine Kosten)
```

### Datenschutz-Vorteile lokaler Modelle
- **Offline-Betrieb:** Keine Internetverbindung fÃ¼r Inference nÃ¶tig
- **Zero Data Transmission:** GesprÃ¤che verlassen niemals den lokalen Rechner
- **DSGVO-Konform:** Keine Ãœbertragung personenbezogener Daten an Dritte
- **Audit-FÃ¤higkeit:** VollstÃ¤ndige Kontrolle Ã¼ber Datenverarbeitung
- **Therapeutische Schweigepflicht:** HÃ¶chster Datenschutz fÃ¼r sensible Inhalte

### Software Dependencies
```python
torch>=2.0.0
transformers>=4.35.0
datasets>=2.14.0
peft>=0.6.0
bitsandbytes>=0.41.0
scikit-learn>=1.3.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
```

## ğŸ“ Projektstruktur

```
therapy-llm-finetuning/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/          # Aufbereitete Datasets
â”‚   â””â”€â”€ results/           # Evaluationsergebnisse
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ finetune_gemma.py  # Haupttraining-Skript
â”‚   â”œâ”€â”€ evaluation.py      # Evaluations-Pipeline
â”‚   â””â”€â”€ utils/             # Hilfsfunktionen
â”œâ”€â”€ models/
â”‚   â””â”€â”€ finetuned-gemma/   # Trainierte Modell-Checkpoints
â”œâ”€â”€ logs/                  # Training & Evaluation Logs
â”œâ”€â”€ notebooks/             # Jupyter Notebooks fÃ¼r Analyse
â”œâ”€â”€ results/               # Plots und Reports
â”œâ”€â”€ requirements.txt       # Python Dependencies
â””â”€â”€ README.md             # Projektbeschreibung
```

## ğŸ” Wissenschaftlicher Beitrag

### Methodische Innovation
- **Quantitative LLM-Evaluation:** Objektive Messung der Finetuning-EffektivitÃ¤t auf therapeutischen Daten
- **Robuste Experimentgestaltung:** Systematischer Vergleich mit echten Baseline-Antworten
- **Lokale AI-Architektur:** Demonstration datenschutzkonformer KI-Systeme fÃ¼r sensible Anwendungen
- **Consumer-Hardware Optimierung:** Beweis der Machbarkeit komplexer AI-Tasks auf Standard-PCs

### Praktische Relevanz fÃ¼r lokale KI-Systeme
- **Datenschutz-Revolution:** Therapeutische KI ohne Cloud-AbhÃ¤ngigkeit
- **Demokratisierung:** KI-Tools zugÃ¤nglich fÃ¼r jeden mit Standard-Gaming-PC
- **Kosteneffizienz:** Einmalige Anschaffung vs. laufende API-Kosten (GPT-4: ~$0.03/1K Tokens)
- **VerfÃ¼gbarkeit:** 24/7 Betrieb ohne InternetabhÃ¤ngigkeit oder Service-AusfÃ¤lle
- **Anpassbarkeit:** VollstÃ¤ndige Kontrolle Ã¼ber Modellverhalten und ethische Richtlinien

### Gesellschaftlicher Impact
- **Therapeutische Versorgung:** UnterstÃ¼tzung in unterversorgten Gebieten
- **Kostenreduktion:** Senkung der Barrieren fÃ¼r mental health support
- **ForschungsfÃ¶rderung:** Open-Source Ansatz fÃ¼r weitere wissenschaftliche Entwicklung
- **Ethik-Standard:** Referenzimplementierung fÃ¼r verantwortliche lokale KI

## ğŸ’¡ Warum lokale Modelle statt Cloud-APIs?

### Datenschutz & Sicherheit
| Aspekt | Cloud-APIs (GPT-4, Claude) | Lokales 4B Modell |
|--------|----------------------------|-------------------|
| **DatenÃ¼bertragung** | Alle Inputs â†’ Cloud Server | Keine (100% lokal) |
| **Datenspeicherung** | Unbekannt/TemporÃ¤r | VollstÃ¤ndige Kontrolle |
| **Compliance** | AbhÃ¤ngig vom Anbieter | DSGVO/HIPAA ready |
| **Auditierbarkeit** | Schwarz-Box | Open Source |
| **Therapeutische Schweigepflicht** | Rechtsunsicherheit | Garantiert erfÃ¼llt |



### VerfÃ¼gbarkeit & Kontrolle
- **24/7 Betrieb:** Keine API-Limits oder Service-AusfÃ¤lle
- **Offline-FÃ¤higkeit:** Funktioniert ohne Internetverbindung
- **Latenz:** Sub-Sekunde Response vs. Cloud-Latenz
- **Anpassbarkeit:** VollstÃ¤ndige Kontrolle Ã¼ber Modellverhalten
- **Updates:** Selbstbestimmte Modell-Updates ohne QualitÃ¤tsverlust


## ğŸš€ AusfÃ¼hrung

### 1. Environment Setup
```bash
pip install -r requirements.txt
```

### 2. Finetuning
```bash
python src/finetune_gemma.py
```

### 3. Evaluation
```bash
python src/evaluation.py
```

### 4. Analyse
```bash
jupyter notebook notebooks/analysis.ipynb
```

## ğŸ“ Limitationen & lokale Modell-RealitÃ¤ten

### Technische Limitationen
- **ModellgrÃ¶ÃŸe:** 4B Parameter vs. 175B+ bei GPT-4 (erwartete QualitÃ¤tsdifferenz)
- **KontextlÃ¤nge:** 8K Token Limit vs. 128K bei neueren Cloud-Modellen
- **MultimodalitÃ¤t:** Text-only vs. Vision+Audio FÃ¤higkeiten grÃ¶ÃŸerer Modelle
- **Training-Daten:** Begrenzte lokale Trainingsdaten (500 Samples) vs. Cloud-Training

### Evaluation-Limitationen  
- **Evaluationsmetrik:** Cosine Similarity erfasst nicht alle Aspekte therapeutischer QualitÃ¤t
- **SubjektivitÃ¤t:** Keine menschliche Bewertung der generierten Antworten
- **Sprache:** Evaluierung nur auf englischsprachigen Daten
- **Kontext:** Evaluation nur auf Einzelturn-GesprÃ¤chen

### Warum diese Limitationen akzeptabel sind
- **Privacy-First:** Datenschutz wiegt QualitÃ¤tsverluste auf
- **Good-Enough:** 80% QualitÃ¤t bei 100% Privacy oft ausreichend
- **Verbesserbar:** Lokale Modelle entwickeln sich schnell weiter
- **Spezialisierung:** Finetuning kann Domain-spezifische Defizite ausgleichen

## ğŸ”® ZukÃ¼nftige Erweiterungen & lokale AI-Roadmap

### Kurzfristig (3-6 Monate)
- **Human Evaluation:** Bewertung durch lizenzierte Therapeuten
- **Multilinguale Evaluation:** Test auf deutsch- und anderssprachigen Daten
- **Hardware-Optimierung:** Benchmarks auf verschiedenen Consumer-GPUs
- **Deployment-Tools:** Docker Container fÃ¼r einfache Installation

### Mittelfristig (6-12 Monate)
- **LÃ¤ngere Dialoge:** Multi-Turn GesprÃ¤che und Session-Memory
- **Lokale RAG-Integration:** Verbindung mit lokalen Wissensdatenbanken
- **Mobile Deployment:** Optimierung fÃ¼r Smartphones/Tablets
- **Federated Learning:** Dezentrale Modellverbesserung ohne Datenteilung

### Langfristig (1-2 Jahre)
- **Lokale MultimodalitÃ¤t:** Integration von Speech-to-Text/Text-to-Speech
- **Edge-Computing:** Deployment auf Raspberry Pi / Edge-Devices
- **Community-Training:** Crowd-sourced Verbesserung lokaler Therapie-Modelle
- **Regulatorische Frameworks:** Standards fÃ¼r lokale medizinische KI-Systeme

### Vision: Demokratisierte therapeutische KI
*"Jeder Mensch sollte Zugang zu hochwertiger, datenschutzkonformer KI-UnterstÃ¼tzung fÃ¼r mental health haben - unabhÃ¤ngig von Internet, Budget oder geografischer Lage."*

---

**Autor:** [David Matischek]  
**Institution:** [Karl Franzens UniversitÃ¤t Graz]  
**Datum:** Juli 2025  
**Kontakt:** [david.matischek@edu.uni-graz.at]
